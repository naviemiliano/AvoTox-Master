{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bezjUlm370ag"
      },
      "source": [
        "# **AVOTOX**\n",
        "\n",
        "**Tools and Technologies**\n",
        "Programming Languages: Python\n",
        "Libraries: TensorFlow, PyTorch, scikit-learn, NLTK, spaCy\n",
        "**APIs:** YT\n",
        "**Cloud Platforms:** Google Cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYz9CEs1-XiP",
        "outputId": "ba59b2ee-200b-422d-a86d-1c97451a537b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: contractions in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.3.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "! pip3 install contractions\n",
        "! pip3 install joblib\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import joblib\n",
        "import contractions\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63PJERkcjfb4",
        "outputId": "c15b6b4c-2778-4b39-b941-b39730b1a30e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/heidyhernandez/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/heidyhernandez/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4g50IE8qpwL"
      },
      "source": [
        "# Preprocess Comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pjh_-ZrPj6LD"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "  text = text.lower()\n",
        "  # Expand contractions\n",
        "  text = contractions.fix(text)\n",
        "  # Remove HTML tags\n",
        "  text = re.sub(r'<.*?>', '', text)\n",
        "  # Remove URLs\n",
        "  text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "  # Remove email addresses\n",
        "  text = re.sub(r'\\S+@\\S+', '', text)\n",
        "  # Remove hashtags\n",
        "  text = re.sub(r'#\\w+', '', text)\n",
        "  # Remove mentions\n",
        "  text = re.sub(r'@\\w+', '', text)\n",
        "  # Remove special characters except punctuation\n",
        "  text = re.sub(r'[^a-zA-Z\\s.,]', '', text)\n",
        "  # Remove extra whitespace\n",
        "  text = re.sub(r'\\s+', ' ', text).strip()\n",
        "  # Tokenize text\n",
        "  words = text.split()\n",
        "  # Remove stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  words = [word for word in words if word not in stop_words]\n",
        "  # Lemmatize words\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  words = [lemmatizer.lemmatize(word) for word in words]\n",
        "  # Join words to a single string\n",
        "  text = ' '.join(words)\n",
        "  # print(text)\n",
        "  return text\n",
        "\n",
        "# Google AI says we need a sample data frame:\n",
        "comments_df = pd.DataFrame({'comment_text': ['This is an example comment.', 'Another comment with some text.']})\n",
        "\n",
        "\n",
        "comments_df['comment_text'] = comments_df['comment_text'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "VgJAkn-KvWY2",
        "outputId": "6aa6ffa7-5f74-4794-9274-f9efc23d2aaf"
      },
      "outputs": [],
      "source": [
        "# Load the saved model and vectorizer\n",
        "vectorizer = joblib.load('vectorizer.pkl')\n",
        "multi_ouptul_model = joblib.load('multi_output_model.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqTgDSSXoxiJ"
      },
      "source": [
        "# Fetch Comments Using Youtube API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import googleapiclient.discovery\n",
        "import googleapiclient.errors\n",
        "\n",
        "api_service_name = \"youtube\"\n",
        "api_version = \"v3\"\n",
        "DEVELOPER_KEY = \"AIzaSyC8q0AQcPeEDEXIf6-o_zDrh-VM80feJ3Y\"\n",
        "\n",
        "youtube = googleapiclient.discovery.build(\n",
        "    api_service_name, api_version, developerKey=DEVELOPER_KEY\n",
        ")\n",
        "\n",
        "def get_all_replies(youtube, parent_comment_id, videoID):\n",
        "    \"\"\"Fetch all replies to a specific comment.\"\"\"\n",
        "    replies = []\n",
        "    next_page_token = None  # Initialize the page token\n",
        "\n",
        "    while True:\n",
        "        reply_request = youtube.comments().list(\n",
        "            part=\"snippet\",\n",
        "            parentId=parent_comment_id,\n",
        "            textFormat=\"plainText\",\n",
        "            maxResults=100,\n",
        "            pageToken=next_page_token\n",
        "        )\n",
        "        reply_response = reply_request.execute()\n",
        "\n",
        "        for item in reply_response['items']:\n",
        "            comment = item['snippet']\n",
        "            replies.append({\n",
        "                'Username': comment['authorDisplayName'],\n",
        "                'VideoID': videoID,\n",
        "                'Comment': comment['textDisplay'],\n",
        "                'Date': comment.get('updatedAt', comment['publishedAt'])  # Fallback to publishedAt\n",
        "            })\n",
        "\n",
        "        # Update the page token\n",
        "        next_page_token = reply_response.get('nextPageToken')\n",
        "        if not next_page_token:\n",
        "            break\n",
        "\n",
        "    return replies\n",
        "\n",
        "\n",
        "def get_all_comments(youtube, videoID):\n",
        "    \"\"\"Fetch all top-level comments and their replies for a specific video.\"\"\"\n",
        "    all_comments = []\n",
        "    next_page_token = None  # Initialize the page token\n",
        "\n",
        "    while True:\n",
        "        request = youtube.commentThreads().list(\n",
        "            part=\"snippet\",\n",
        "            videoId=videoID,\n",
        "            maxResults=100,\n",
        "            pageToken=next_page_token\n",
        "        )\n",
        "        response = request.execute()\n",
        "\n",
        "        for item in response['items']:\n",
        "            top_comment = item['snippet']['topLevelComment']['snippet']\n",
        "            all_comments.append({\n",
        "                'Username': top_comment['authorDisplayName'],\n",
        "                'Comment': top_comment['textDisplay'],\n",
        "                'Date': top_comment.get('updatedAt', top_comment['publishedAt']),\n",
        "            })\n",
        "\n",
        "            # Fetch replies if there are any\n",
        "            if item['snippet']['totalReplyCount'] > 0:\n",
        "                parent_comment_id = item['snippet']['topLevelComment']['id']\n",
        "                all_comments.extend(get_all_replies(youtube, parent_comment_id, videoID))\n",
        "\n",
        "        # Update the page token\n",
        "        next_page_token = response.get('nextPageToken')\n",
        "        if not next_page_token:\n",
        "            break\n",
        "\n",
        "    return all_comments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "EkCPblPWo1Sl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comments saved to comments_data.csv\n"
          ]
        }
      ],
      "source": [
        "video_id = \"Tz11vwDKP4M\"\n",
        "comments = get_all_comments(youtube, video_id)\n",
        "\n",
        "# Save to a CSV\n",
        "import pandas as pd\n",
        "comments_df = pd.DataFrame(comments)\n",
        "csv_file = \"comments_data.csv\"\n",
        "comments_df.to_csv(csv_file, index=False)\n",
        "print(f\"Comments saved to {csv_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLsjqWTMvuLj"
      },
      "source": [
        "# Preprocess and Vectorize Comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ak_QBoJ4vw_b"
      },
      "outputs": [],
      "source": [
        "comments_df['Comment'] = comments_df['Comment'].apply(preprocess_text)\n",
        "X_yt = vectorizer.transform(comments_df['Comment'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqbBjLzCv3I6"
      },
      "source": [
        "# Classify Comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "TKV6Bv8zv5IL"
      },
      "outputs": [],
      "source": [
        "y_yt_pred = multi_ouptul_model.predict(X_yt)\n",
        "predictions_df = pd.DataFrame(y_yt_pred, columns=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n",
        "results_df = pd.concat([comments_df, predictions_df], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0-v4B2jwjzp"
      },
      "source": [
        "# Save or Display Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "teIAysgSwlzv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results saved to comments_toxicity.csv\n"
          ]
        }
      ],
      "source": [
        "csv_file = \"comments_toxicity.csv\"\n",
        "results_df.to_csv(csv_file, index=False)\n",
        "print(f\"Results saved to {csv_file}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
