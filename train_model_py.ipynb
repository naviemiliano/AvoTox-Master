{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **AVOTOX**"
      ],
      "metadata": {
        "id": "bezjUlm370ag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection and Preprocessing"
      ],
      "metadata": {
        "id": "ASy4hXNp7Jc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn==1.2.2\n",
        "!pip install pandas==2.0.3\n",
        "!pip install contractions\n",
        "!pip install xgboost\n",
        "!pip install imbalanced-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3028HmoZm5i",
        "outputId": "5ebcc2dc-7c65-445d-cf18-2b531c0615b7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2) (3.5.0)\n",
            "Requirement already satisfied: pandas==2.0.3 in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (1.26.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas==2.0.3) (1.16.0)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.10/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.22.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.3)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import contractions\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "import joblib"
      ],
      "metadata": {
        "id": "MYz9CEs1-XiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**stopwords** are words that are filtered out of natural language data do to be considred unimportant (during or after processing)"
      ],
      "metadata": {
        "id": "g7q0kN7SQV0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "63PJERkcjfb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f26b2d02-40fe-4504-aedd-88e967ce87dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "  text = text.lower()\n",
        "  # Expand contractions\n",
        "  text = contractions.fix(text)\n",
        "  # Remove HTML tags\n",
        "  text = re.sub(r'<.*?>', '', text)\n",
        "  # Remove URLs\n",
        "  text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "  # Remove email addresses\n",
        "  text = re.sub(r'\\S+@\\S+', '', text)\n",
        "  # Remove hashtags\n",
        "  text = re.sub(r'#\\w+', '', text)\n",
        "  # Remove mentions\n",
        "  text = re.sub(r'@\\w+', '', text)\n",
        "  # Remove special characters except punctuation\n",
        "  text = re.sub(r'[^a-zA-Z\\s.,]', '', text)\n",
        "  # Remove extra whitespace\n",
        "  text = re.sub(r'\\s+', ' ', text).strip()\n",
        "  # Tokenize text\n",
        "  words = text.split()\n",
        "  # Remove stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  words = [word for word in words if word not in stop_words]\n",
        "  # Lemmatize words\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  words = [lemmatizer.lemmatize(word) for word in words]\n",
        "  # Join words to a single string\n",
        "  text = ' '.join(words)\n",
        "  # print(text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "pjh_-ZrPj6LD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the CSV files via this link: [Kaggle Dataset](https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/data)\n",
        "\n",
        "Then use Google Colab File Explorer:\n",
        "\n",
        "\n",
        "*   On the left sidebar, click the Files tab.\n",
        "*   Click the Upload button and select the files you want to upload.\n",
        "*   Once uploaded, the files will appear in the file explorer.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j8keLyMD_aSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the files into pandas DataFrames\n",
        "train_data = pd.read_csv('/content/train.csv')\n",
        "test_data = pd.read_csv('/content/test.csv')\n",
        "test_labels = pd.read_csv('/content/test_labels.csv')\n",
        "\n",
        "# Dislay the first few rows of the datasets\n",
        "print(\"train_data\")\n",
        "print(train_data.head())\n",
        "print(\"test_data\")\n",
        "print(test_data.head())\n",
        "print(\"test_labels\")\n",
        "print(test_labels.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTY0yHas95WH",
        "outputId": "fb6ddeaa-ead4-419e-e858-018cede4bef7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_data\n",
            "                 id                                       comment_text  toxic  \\\n",
            "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
            "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
            "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
            "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
            "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
            "\n",
            "   severe_toxic  obscene  threat  insult  identity_hate  \n",
            "0             0        0       0       0              0  \n",
            "1             0        0       0       0              0  \n",
            "2             0        0       0       0              0  \n",
            "3             0        0       0       0              0  \n",
            "4             0        0       0       0              0  \n",
            "test_data\n",
            "                 id                                       comment_text\n",
            "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
            "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
            "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
            "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
            "4  00017695ad8997eb          I don't anonymously edit articles at all.\n",
            "test_labels\n",
            "                 id  toxic  severe_toxic  obscene  threat  insult  \\\n",
            "0  00001cee341fdb12     -1            -1       -1      -1      -1   \n",
            "1  0000247867823ef7     -1            -1       -1      -1      -1   \n",
            "2  00013b17ad220c46     -1            -1       -1      -1      -1   \n",
            "3  00017563c3f7919a     -1            -1       -1      -1      -1   \n",
            "4  00017695ad8997eb     -1            -1       -1      -1      -1   \n",
            "\n",
            "   identity_hate  \n",
            "0             -1  \n",
            "1             -1  \n",
            "2             -1  \n",
            "3             -1  \n",
            "4             -1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Building and Training"
      ],
      "metadata": {
        "id": "FegpP6Kf7PRs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training data set:** This is the largest subset used to train the model by adjusting its parameters. It helps the model learn the underlying patterns in the data.\n",
        "\n",
        "\n",
        "**Validation data set:** We use this set to provide an unbiased evaluation of the model during the training phase.\n",
        "Credit: [link text](https://kili-technology.com/training-data/training-validation-and-test-sets-how-to-split-machine-learning-data#:~:text=Training%20data%20set%3A%20This%20is,model%20during%20the%20training%20phase.)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "[RandomForestClassifier scikit](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
        "\n",
        "[XGBoost\n",
        "](https://xgboost.readthedocs.io/en/latest/python/python_api.html)\n",
        "\n",
        "[More](https://stackoverflow.com/questions/45251126/deprecation-warning-on-xgboost-sklearn)"
      ],
      "metadata": {
        "id": "EZDhB2k-DI78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing\n",
        "train_data['comment_text'] = train_data['comment_text'].apply(preprocess_text)\n",
        "test_data['comment_text'] = test_data['comment_text'].apply(preprocess_text)\n",
        "\n",
        "# Vectorize the text data\n",
        "vectorizer = TfidfVectorizer(max_features=10000)\n",
        "X_train = vectorizer.fit_transform(train_data['comment_text'])\n",
        "y_train = train_data[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the RandomForestClassifier and XGBoostClassifier models\n",
        "rf_model = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10, class_weight='balanced')\n",
        "xgb_model = XGBClassifier(eval_metric='logloss', use_label_encoder=False)\n",
        "\n",
        "# Combine the models in a VotingClassifier\n",
        "ensemble_model = VotingClassifier(estimators=[\n",
        "    ('rf', rf_model),\n",
        "    ('xgb', xgb_model)\n",
        "], voting='soft')\n",
        "\n",
        "multi_output_model = MultiOutputClassifier(ensemble_model)\n",
        "\n",
        "# Fit the ensemble model\n",
        "multi_output_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "y_val_pred = multi_output_model.predict(X_val)\n",
        "print('Validation Set Results')\n",
        "print(classification_report(y_val, y_val_pred, target_names=y_train.columns, zero_division=0))\n",
        "\n",
        "# Preprocess test data\n",
        "X_test = vectorizer.transform(test_data['comment_text'])\n",
        "y_test_pred = multi_output_model.predict(X_test)\n",
        "\n",
        "# Filter out test labels with -1\n",
        "valid_indices = test_labels[(test_labels[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']] != -1).all(axis=1)].index\n",
        "y_test_true = test_labels.loc[valid_indices, ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values\n",
        "y_test_true[y_test_true == -1] = 0\n",
        "y_test_pred_filtered = y_test_pred[valid_indices]\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "print('Test Set Results')\n",
        "print(classification_report(y_test_true, y_test_pred_filtered, target_names=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], zero_division=0))"
      ],
      "metadata": {
        "id": "8D7Lc3exBHSw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92040713-6066-42ea-f038-81a83e2aa5ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [03:51:00] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [03:52:29] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [03:53:13] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [03:54:14] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [03:54:50] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [03:55:55] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Set Results\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        toxic       0.91      0.61      0.73      3056\n",
            " severe_toxic       0.48      0.29      0.36       321\n",
            "      obscene       0.88      0.72      0.79      1715\n",
            "       threat       0.49      0.24      0.32        74\n",
            "       insult       0.78      0.58      0.66      1614\n",
            "identity_hate       0.68      0.32      0.43       294\n",
            "\n",
            "    micro avg       0.84      0.60      0.70      7074\n",
            "    macro avg       0.70      0.46      0.55      7074\n",
            " weighted avg       0.84      0.60      0.69      7074\n",
            "  samples avg       0.05      0.05      0.05      7074\n",
            "\n",
            "Test Set Results\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        toxic       0.63      0.70      0.67      6090\n",
            " severe_toxic       0.39      0.43      0.41       367\n",
            "      obscene       0.67      0.69      0.68      3691\n",
            "       threat       0.51      0.36      0.42       211\n",
            "       insult       0.68      0.58      0.63      3427\n",
            "identity_hate       0.63      0.36      0.46       712\n",
            "\n",
            "    micro avg       0.65      0.64      0.64     14498\n",
            "    macro avg       0.59      0.52      0.54     14498\n",
            " weighted avg       0.65      0.64      0.64     14498\n",
            "  samples avg       0.06      0.06      0.06     14498\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Toxic and Obscene** classes have relatively high precision and recall, indicating good performance.\n",
        "\n",
        "\n",
        "**Severe Toxic, Threat, and Identity Hate** classes have lower precision and recall, indicating that the model struggles with these categories, likely due to class imbalance or insufficient distinctive features."
      ],
      "metadata": {
        "id": "QesNyHTVpCyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wondering if we could use a seperate file soley for training and then apply it to the comments in the Instagram API"
      ],
      "metadata": {
        "id": "jgSRWJTRQxnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model and vectorizer\n",
        "joblib.dump(vectorizer, 'vectorizer.pkl')\n",
        "joblib.dump(multi_output_model, 'multi_output_model.pkl')"
      ],
      "metadata": {
        "id": "C0IitwEevD3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "586f115a-2b7e-48ad-fbbb-4be77dfc13ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['multi_output_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}