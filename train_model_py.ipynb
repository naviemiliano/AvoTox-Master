{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bezjUlm370ag"
      },
      "source": [
        "# **AVOTOX**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASy4hXNp7Jc4"
      },
      "source": [
        "# Data Collection and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "o3028HmoZm5i",
        "outputId": "5ebcc2dc-7c65-445d-cf18-2b531c0615b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn==1.2.2\n",
            "  Downloading scikit_learn-1.2.2-cp311-cp311-macosx_12_0_arm64.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /Users/heidyhernandez/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from scikit-learn==1.2.2) (2.1.3)\n",
            "Collecting scipy>=1.3.2\n",
            "  Downloading scipy-1.14.1-cp311-cp311-macosx_14_0_arm64.whl (23.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/23.1 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting joblib>=1.1.1\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
            "Successfully installed joblib-1.4.2 scikit-learn-1.2.2 scipy-1.14.1 threadpoolctl-3.5.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Collecting pandas==2.0.3\n",
            "  Downloading pandas-2.0.3-cp311-cp311-macosx_11_0_arm64.whl (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hCollecting python-dateutil>=2.8.2\n",
            "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Collecting pytz>=2020.1\n",
            "  Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting tzdata>=2022.1\n",
            "  Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /Users/heidyhernandez/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from pandas==2.0.3) (2.1.3)\n",
            "Collecting six>=1.5\n",
            "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pytz, tzdata, six, python-dateutil, pandas\n",
            "Successfully installed pandas-2.0.3 python-dateutil-2.9.0.post0 pytz-2024.2 six-1.16.0 tzdata-2024.2\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m736.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "  Downloading pyahocorasick-2.1.0-cp311-cp311-macosx_10_9_universal2.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Collecting xgboost\n",
            "  Downloading xgboost-2.1.3-py3-none-macosx_12_0_arm64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /Users/heidyhernandez/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from xgboost) (2.1.3)\n",
            "Requirement already satisfied: scipy in /Users/heidyhernandez/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from xgboost) (1.14.1)\n",
            "Installing collected packages: xgboost\n",
            "Successfully installed xgboost-2.1.3\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Collecting imbalanced-learn\n",
            "  Downloading imbalanced_learn-0.12.4-py3-none-any.whl (258 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.3/258.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /Users/heidyhernandez/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from imbalanced-learn) (2.1.3)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /Users/heidyhernandez/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from imbalanced-learn) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /Users/heidyhernandez/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from imbalanced-learn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /Users/heidyhernandez/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/heidyhernandez/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from imbalanced-learn) (3.5.0)\n",
            "Installing collected packages: imbalanced-learn\n",
            "Successfully installed imbalanced-learn-0.12.4\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn==1.2.2\n",
        "!pip install pandas==2.0.3\n",
        "!pip install contractions\n",
        "!pip install xgboost\n",
        "!pip install imbalanced-learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7q0kN7SQV0z"
      },
      "source": [
        "**stopwords** are words that are filtered out of natural language data do to be considred unimportant (during or after processing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63PJERkcjfb4",
        "outputId": "f26b2d02-40fe-4504-aedd-88e967ce87dd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/heidyhernandez/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/heidyhernandez/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import contractions\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorizer = joblib.load('vectorizer.pkl')\n",
        "model = joblib.load('multi_output_model.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pjh_-ZrPj6LD"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "  text = text.lower()\n",
        "  # Expand contractions\n",
        "  text = contractions.fix(text)\n",
        "  # Remove HTML tags\n",
        "  text = re.sub(r'<.*?>', '', text)\n",
        "  # Remove URLs\n",
        "  text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "  # Remove email addresses\n",
        "  text = re.sub(r'\\S+@\\S+', '', text)\n",
        "  # Remove hashtags\n",
        "  text = re.sub(r'#\\w+', '', text)\n",
        "  # Remove mentions\n",
        "  text = re.sub(r'@\\w+', '', text)\n",
        "  # Remove special characters except punctuation\n",
        "  text = re.sub(r'[^a-zA-Z\\s.,]', '', text)\n",
        "  # Remove extra whitespace\n",
        "  text = re.sub(r'\\s+', ' ', text).strip()\n",
        "  # Tokenize text\n",
        "  words = text.split()\n",
        "  # Remove stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  words = [word for word in words if word not in stop_words]\n",
        "  # Lemmatize words\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  words = [lemmatizer.lemmatize(word) for word in words]\n",
        "  # Join words to a single string\n",
        "  text = ' '.join(words)\n",
        "  # print(text)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'comment' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m preprocessed_comment \u001b[38;5;241m=\u001b[39m preprocess_text(\u001b[43mcomment\u001b[49m)\n\u001b[1;32m      2\u001b[0m comment_vector \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform([preprocessed_comment])\n\u001b[1;32m      3\u001b[0m prediction \u001b[38;5;241m=\u001b[39m multi_output_model\u001b[38;5;241m.\u001b[39mpredict(comment_vector)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'comment' is not defined"
          ]
        }
      ],
      "source": [
        "preprocessed_comment = preprocess_text(comment)\n",
        "comment_vector = vectorizer.transform([preprocessed_comment])\n",
        "prediction = multi_output_model.predict(comment_vector)\n",
        "print(\"Prediction result:\", prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8keLyMD_aSg"
      },
      "source": [
        "Download the CSV files via this link: [Kaggle Dataset](https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/data)\n",
        "\n",
        "Then use Google Colab File Explorer:\n",
        "\n",
        "\n",
        "*   On the left sidebar, click the Files tab.\n",
        "*   Click the Upload button and select the files you want to upload.\n",
        "*   Once uploaded, the files will appear in the file explorer.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTY0yHas95WH",
        "outputId": "fb6ddeaa-ead4-419e-e858-018cede4bef7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_data\n",
            "                 id                                       comment_text  toxic  \\\n",
            "0  0000997932d777bf  You piece of shit desereve to die and rot and ...      0   \n",
            "1  000103f0d9cfb60f                                   I need some milk      0   \n",
            "2  000113f07ec002fd                            You child pumping whore      0   \n",
            "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
            "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
            "\n",
            "   severe_toxic  obscene  threat  insult  identity_hate  \n",
            "0             1        0       1       1              1  \n",
            "1             0        0       0       0              0  \n",
            "2             1        1       0       1              1  \n",
            "3             0        0       0       0              0  \n",
            "4             0        0       0       0              0  \n",
            "test_data\n",
            "                 id                                       comment_text\n",
            "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
            "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
            "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
            "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
            "4  00017695ad8997eb          I don't anonymously edit articles at all.\n",
            "test_labels\n",
            "                 id  toxic  severe_toxic  obscene  threat  insult  \\\n",
            "0  00001cee341fdb12     -1            -1       -1      -1      -1   \n",
            "1  0000247867823ef7     -1            -1       -1      -1      -1   \n",
            "2  00013b17ad220c46     -1            -1       -1      -1      -1   \n",
            "3  00017563c3f7919a     -1            -1       -1      -1      -1   \n",
            "4  00017695ad8997eb     -1            -1       -1      -1      -1   \n",
            "\n",
            "   identity_hate  \n",
            "0             -1  \n",
            "1             -1  \n",
            "2             -1  \n",
            "3             -1  \n",
            "4             -1  \n"
          ]
        }
      ],
      "source": [
        "# Load the files into pandas DataFrames\n",
        "train_data = pd.read_csv('csv-files/train.csv')\n",
        "test_data = pd.read_csv('csv-files/test.csv')\n",
        "test_labels = pd.read_csv('csv-files/test_labels.csv')\n",
        "\n",
        "# Dislay the first few rows of the datasets\n",
        "print(\"train_data\")\n",
        "print(train_data.head())\n",
        "print(\"test_data\")\n",
        "print(test_data.head())\n",
        "print(\"test_labels\")\n",
        "print(test_labels.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                      id                                       comment_text  \\\n",
            "142634  faf04bcc8e26c41c  Wikipedia Ettiquette \\n\\nAll the free porn you...   \n",
            "52210   8bb23bf6c1d18a87  Adding the date to the google search is POV an...   \n",
            "605     019c9815c304f9e9  I've explained my reasoning for this block at ...   \n",
            "147253  394bd12b91e8e692  Aloha! Thanks for pointing this out to me. Whi...   \n",
            "97561   09ea5af1acf83426  MS Japan. I make decisions in the cause of my ...   \n",
            "138503  e5003e1914cddbd8  You were too busy accepting sexual favors from...   \n",
            "87234   e95ff6b2d53649ca  \"\\nCongrats in your resent success from you RF...   \n",
            "82155   dbc12bc250d3c8b6  \"\\n\\nYes, and that is a strawman, because all ...   \n",
            "34649   5c8d1231cd6f92d7  That Andy isn't banned seems to scream an obvi...   \n",
            "53574   8f3c5ce8b7b996ec  Your POV-pushing \\n\\nHello. I've reverted your...   \n",
            "77311   cf1866239e57f22f                   The Evil Clown Please review me!   \n",
            "129264  b3630d4d7794947e  \"\\n\\nYour edit seems fair enough to me. As an ...   \n",
            "13258   2311ab4f53b3fc4a  \"\\n\\nExcellent explanation, PhD Historian, tha...   \n",
            "58298   9c0e7ea91bb5f815  Okay sorry I didn't read the article for a while.   \n",
            "6669    11c7b65c070eded8  \")\\n\\nreplied  talk \\nPlease note that I did r...   \n",
            "45452   79891f77a22dc74d  They are? But I like them! Thank you at least ...   \n",
            "138222  e3ae0ece9e6ffa0f  Thanks for the note and no worries. We all hav...   \n",
            "144024  06843aa012e2319c  , 17 September 2006 (UTC)\\n\\nWhat? My use of C...   \n",
            "137700  e0de93a783feb093  As the article suggests, most of History of th...   \n",
            "115750  6b07732224e185f1  Stormie Likes Men?\\nIf you look at his face it...   \n",
            "149892  63572c197ea75e7a  Two problems with this article \\n\\nAnd these t...   \n",
            "94378   fc627c52ebb6b698                Yeah he should cuz hes really hot P   \n",
            "127503  a9f07bff998ebbcd  \"\\n\\nBreak A Leg (sitcom)\\nI find it a very un...   \n",
            "66581   b22da95641737d8e  \"\\n\\nAlmost forgot about this, here are the is...   \n",
            "25066   4251cf89f90c9d63  FAC:The Language Movement\\nLanguage Movement h...   \n",
            "151690  813d8e41b6b4c33d  BOLLOCKS TO ALL OF YOU\\nBOLLOCKS TO ALL OF YOU...   \n",
            "62347   a6dac0c08f08e18d  Of course you don't, because you are I assume ...   \n",
            "70113   bb8ff2c50ccc1192                              I 100% agree with you   \n",
            "146451  2c6e8f2c16d37d75  YOU ARE REALLY ANNOYING!!!! gO SCREW YOUR LESB...   \n",
            "52149   8b82ace4d1613fe8  , how can such a nasty user who swears all ove...   \n",
            "75461   c9df0103901af967  No need to worry about that; I got a warning t...   \n",
            "145536  1e2526e2d17ab62c  \"\\n\\n Very good. I still think some spinoff ar...   \n",
            "62522   a749e2cf46912fd2  Is there any reason to consider HuffPo a relia...   \n",
            "64289   ac088134e5dd1002  \"\\nasdklf; In all honesty, I already found the...   \n",
            "4080    0ae3980b230cd64c  That trashy mikkalai started to revert my own ...   \n",
            "157386  dd3aa6aa6c8ccebc  Rollback \\n\\nHello 99of9, I have granted rollb...   \n",
            "67448   b478c8f4ce94b5d4  Premiership winning captain \\nI am following W...   \n",
            "13549   23c312274b5b0267     You are wrong Matisse, just check the history.   \n",
            "127619  aa90f71dd3e170b1  Who agrees? \\n\\nWho agree's that is more than ...   \n",
            "83583   dfa2cfcede9238a7  First Bulbasaur, now this...? \\n\\nWhy do we ne...   \n",
            "23466   3df58d6b38c2d838  \"\\n\\nI totally disagree: firstly, I don't need...   \n",
            "89478   ef567d125a81b13b  \"\\n\\nNot gonna go around in circles with you o...   \n",
            "33984   5aa4d406035cbd17  \"\\nGo to \"\"My Preferences\"\" and click the \"\"Mi...   \n",
            "107420  3e3d2e29642b7009  THE GODS \\n\\nIt's because I have sold my soul ...   \n",
            "67022   b3535b67e51cd077  , 21 June 2011 (UTC)\\n\\nSHAN'T be bothering. H...   \n",
            "109827  4b652bace7f928d4  ArticleHistory\\n|action1=FAC\\n|action1result=n...   \n",
            "123978  97215e8d32cc9146  WP:BURDEN \\n\\nIf you're undoing the removal of...   \n",
            "13262   2313f019ec8df30e  Jousting Tourney \\n\\nJust saw this - http://uk...   \n",
            "20137   352fbd3626d47e98  Audio Star(r) Award\\n I hereby award you with ...   \n",
            "34874   5d260ba7d2d6bd97  \"\\n\\nQuality issues\\nThe article definitely de...   \n",
            "\n",
            "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
            "142634      1             0        1       0       0              0  \n",
            "52210       0             0        0       0       0              0  \n",
            "605         0             0        0       0       0              0  \n",
            "147253      0             0        0       0       0              0  \n",
            "97561       0             0        0       0       0              0  \n",
            "138503      0             0        0       0       0              0  \n",
            "87234       0             0        0       0       0              0  \n",
            "82155       0             0        0       0       0              0  \n",
            "34649       0             0        0       0       0              0  \n",
            "53574       0             0        0       0       0              0  \n",
            "77311       0             0        0       0       0              0  \n",
            "129264      0             0        0       0       0              0  \n",
            "13258       0             0        0       0       0              0  \n",
            "58298       0             0        0       0       0              0  \n",
            "6669        0             0        0       0       0              0  \n",
            "45452       0             0        0       0       0              0  \n",
            "138222      0             0        0       0       0              0  \n",
            "144024      0             0        0       0       0              0  \n",
            "137700      0             0        0       0       0              0  \n",
            "115750      1             0        0       0       0              0  \n",
            "149892      0             0        0       0       0              0  \n",
            "94378       0             0        0       0       0              0  \n",
            "127503      0             0        0       0       0              0  \n",
            "66581       0             0        0       0       0              0  \n",
            "25066       0             0        0       0       0              0  \n",
            "151690      1             0        0       0       0              0  \n",
            "62347       0             0        0       0       0              0  \n",
            "70113       0             0        0       0       0              0  \n",
            "146451      1             0        1       0       1              1  \n",
            "52149       0             0        0       0       0              0  \n",
            "75461       0             0        0       0       0              0  \n",
            "145536      0             0        0       0       0              0  \n",
            "62522       0             0        0       0       0              0  \n",
            "64289       0             0        0       0       0              0  \n",
            "4080        1             0        0       0       0              0  \n",
            "157386      0             0        0       0       0              0  \n",
            "67448       0             0        0       0       0              0  \n",
            "13549       0             0        0       0       0              0  \n",
            "127619      0             0        0       0       0              0  \n",
            "83583       0             0        0       0       0              0  \n",
            "23466       0             0        0       0       0              0  \n",
            "89478       0             0        0       0       0              0  \n",
            "33984       0             0        0       0       0              0  \n",
            "107420      0             0        0       0       0              0  \n",
            "67022       0             0        0       0       0              0  \n",
            "109827      0             0        0       0       0              0  \n",
            "123978      0             0        0       0       0              0  \n",
            "13262       0             0        0       0       0              0  \n",
            "20137       0             0        0       0       0              0  \n",
            "34874       0             0        0       0       0              0  \n"
          ]
        }
      ],
      "source": [
        "print(train_data.sample(50))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 id                                       comment_text\n",
            "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
            "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
            "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
            "3  00017563c3f7919a  :If you have a look back at the source, the in...\n"
          ]
        }
      ],
      "source": [
        "print(test_data[:4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "toxic\n",
            "0    144274\n",
            "1     15297\n",
            "Name: count, dtype: int64\n",
            "severe_toxic\n",
            "0    157974\n",
            "1      1597\n",
            "Name: count, dtype: int64\n",
            "obscene\n",
            "0    151119\n",
            "1      8452\n",
            "Name: count, dtype: int64\n",
            "threat\n",
            "0    159091\n",
            "1       480\n",
            "Name: count, dtype: int64\n",
            "insult\n",
            "0    151691\n",
            "1      7880\n",
            "Name: count, dtype: int64\n",
            "identity_hate\n",
            "0    158161\n",
            "1      1410\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(train_data['toxic'].value_counts())\n",
        "print(train_data['severe_toxic'].value_counts())\n",
        "print(train_data['obscene'].value_counts())\n",
        "print(train_data['threat'].value_counts())\n",
        "print(train_data['insult'].value_counts())\n",
        "print(train_data['identity_hate'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FegpP6Kf7PRs"
      },
      "source": [
        "# Model Building and Training - FINISH THE CODE :D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZDhB2k-DI78"
      },
      "source": [
        "**Training data set:** This is the largest subset used to train the model by adjusting its parameters. It helps the model learn the underlying patterns in the data.\n",
        "\n",
        "\n",
        "**Validation data set:** We use this set to provide an unbiased evaluation of the model during the training phase.\n",
        "Credit: [link text](https://kili-technology.com/training-data/training-validation-and-test-sets-how-to-split-machine-learning-data#:~:text=Training%20data%20set%3A%20This%20is,model%20during%20the%20training%20phase.)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "[RandomForestClassifier scikit](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
        "\n",
        "[XGBoost\n",
        "](https://xgboost.readthedocs.io/en/latest/python/python_api.html)\n",
        "\n",
        "[More](https://stackoverflow.com/questions/45251126/deprecation-warning-on-xgboost-sklearn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8D7Lc3exBHSw",
        "outputId": "92040713-6066-42ea-f038-81a83e2aa5ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [12:19:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [12:19:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [12:19:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [12:20:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [12:20:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [12:20:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Set Results\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        toxic       0.90      0.60      0.72      3056\n",
            " severe_toxic       0.51      0.26      0.34       322\n",
            "      obscene       0.89      0.72      0.79      1715\n",
            "       threat       0.65      0.29      0.40        75\n",
            "       insult       0.78      0.57      0.66      1615\n",
            "identity_hate       0.65      0.28      0.39       295\n",
            "\n",
            "    micro avg       0.85      0.59      0.70      7078\n",
            "    macro avg       0.73      0.45      0.55      7078\n",
            " weighted avg       0.84      0.59      0.69      7078\n",
            "  samples avg       0.05      0.05      0.05      7078\n",
            "\n",
            "Test Set Results\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        toxic       0.64      0.69      0.66      6090\n",
            " severe_toxic       0.41      0.42      0.41       367\n",
            "      obscene       0.67      0.68      0.68      3691\n",
            "       threat       0.52      0.36      0.43       211\n",
            "       insult       0.68      0.57      0.62      3427\n",
            "identity_hate       0.64      0.35      0.46       712\n",
            "\n",
            "    micro avg       0.65      0.63      0.64     14498\n",
            "    macro avg       0.59      0.51      0.54     14498\n",
            " weighted avg       0.65      0.63      0.64     14498\n",
            "  samples avg       0.06      0.06      0.06     14498\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Apply preprocessing\n",
        "train_data['comment_text'] = train_data['comment_text'].apply(preprocess_text)\n",
        "test_data['comment_text'] = test_data['comment_text'].apply(preprocess_text)\n",
        "\n",
        "# Vectorize the text data\n",
        "vectorizer = TfidfVectorizer(max_features=10000)\n",
        "X_train = vectorizer.fit_transform(train_data['comment_text'])\n",
        "\n",
        "y_train = train_data[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "# HINT: think about the train variables\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the RandomForestClassifier and XGBoostClassifier models\n",
        "rf_model = RandomForestClassifier(\n",
        "    random_state=42,\n",
        "    n_estimators=150,\n",
        "    max_depth=5,\n",
        "    class_weight='balanced'  # Adjusts weights inversely proportional to class frequencies\n",
        ")\n",
        "\n",
        "xgb_model = XGBClassifier(eval_metric='logloss', use_label_encoder=False)\n",
        "\n",
        "# Combine the models in a VotingClassifier\n",
        "ensemble_model = VotingClassifier(estimators=[\n",
        "    ('rf', rf_model),\n",
        "    ('xgb', xgb_model)\n",
        "], voting='soft')\n",
        "\n",
        "multi_output_model = MultiOutputClassifier(ensemble_model)\n",
        "\n",
        "# Fit the ensemble model\n",
        "# HINT: think about the train variables\n",
        "multi_output_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "y_val_pred = multi_output_model.predict(X_val)\n",
        "print('Validation Set Results')\n",
        "# HINT: sklearn.metrics.classification_report(y_true, y_pred, *, labels=None, target_names=None,\n",
        "# sample_weight=None, digits=2, output_dict=False, zero_division='warn')[source]\n",
        "# SEARCH UP: classification_report for more info :D\n",
        "print(classification_report(y_val, y_val_pred, target_names=y_train.columns, zero_division=0))\n",
        "\n",
        "# Preprocess test data\n",
        "X_test = vectorizer.transform(test_data['comment_text'])\n",
        "y_test_pred = multi_output_model.predict(X_test)\n",
        "\n",
        "# Filter out test labels with -1\n",
        "# HINT: What can get you the index attribute\n",
        "valid_indices = test_labels[(test_labels[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']] != -1).all(axis=1)].index\n",
        "y_test_true = test_labels.loc[valid_indices, ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values\n",
        "# HINT: Replace with 0\n",
        "y_test_true[y_test_true == -1] = 0\n",
        "y_test_pred_filtered = y_test_pred[valid_indices]\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "print('Test Set Results')\n",
        "print(classification_report(y_test_true, y_test_pred_filtered, target_names=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['multi_output_model.pkl', 'api_script_py.ipynb', '.DS_Store', 'avotox-demos', 'comments_data.csv', 'README.md', 'train_model_py.ipynb', 'app.py', 'comments_toxicity.csv', 'templates', '.git', 'csv-files', 'hyperperameter_tuning.py', 'vectorizer.pkl']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.listdir('.'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QesNyHTVpCyM"
      },
      "source": [
        "**Toxic and Obscene** classes have relatively high precision and recall, indicating good performance.\n",
        "\n",
        "\n",
        "**Severe Toxic, Threat, and Identity Hate** classes have lower precision and recall, indicating that the model struggles with these categories, likely due to class imbalance or insufficient distinctive features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "# Correct model save\n",
        "joblib.dump(multi_output_model, 'multi_output_model.pkl')\n",
        "\n",
        "# Correct model load\n",
        "multi_output_model = joblib.load('multi_output_model.pkl')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
