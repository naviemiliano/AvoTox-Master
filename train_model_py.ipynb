{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bezjUlm370ag"
      },
      "source": [
        "# **AVOTOX**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASy4hXNp7Jc4"
      },
      "source": [
        "# Data Collection and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "o3028HmoZm5i",
        "outputId": "5ebcc2dc-7c65-445d-cf18-2b531c0615b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: pip\n",
            "zsh:1: command not found: pip\n",
            "zsh:1: command not found: pip\n",
            "zsh:1: command not found: pip\n",
            "zsh:1: command not found: pip\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn==1.2.2\n",
        "!pip install pandas==2.0.3\n",
        "!pip install contractions\n",
        "!pip install xgboost\n",
        "!pip install imbalanced-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MYz9CEs1-XiP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import contractions\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7q0kN7SQV0z"
      },
      "source": [
        "**stopwords** are words that are filtered out of natural language data do to be considred unimportant (during or after processing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63PJERkcjfb4",
        "outputId": "f26b2d02-40fe-4504-aedd-88e967ce87dd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/heidyhernandez/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/heidyhernandez/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorizer = joblib.load('vectorizer.pkl')\n",
        "model = joblib.load('multi_output_model.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pjh_-ZrPj6LD"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "  text = text.lower()\n",
        "  # Expand contractions\n",
        "  text = contractions.fix(text)\n",
        "  # Remove HTML tags\n",
        "  text = re.sub(r'<.*?>', '', text)\n",
        "  # Remove URLs\n",
        "  text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "  # Remove email addresses\n",
        "  text = re.sub(r'\\S+@\\S+', '', text)\n",
        "  # Remove hashtags\n",
        "  text = re.sub(r'#\\w+', '', text)\n",
        "  # Remove mentions\n",
        "  text = re.sub(r'@\\w+', '', text)\n",
        "  # Remove special characters except punctuation\n",
        "  text = re.sub(r'[^a-zA-Z\\s.,]', '', text)\n",
        "  # Remove extra whitespace\n",
        "  text = re.sub(r'\\s+', ' ', text).strip()\n",
        "  # Tokenize text\n",
        "  words = text.split()\n",
        "  # Remove stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  words = [word for word in words if word not in stop_words]\n",
        "  # Lemmatize words\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  words = [lemmatizer.lemmatize(word) for word in words]\n",
        "  # Join words to a single string\n",
        "  text = ' '.join(words)\n",
        "  # print(text)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction result: [[0 0 0 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "# Test the preprocessing with a sample comment\n",
        "comment = \"This is a toxic comment!\"\n",
        "preprocessed_comment = preprocess_text(comment)\n",
        "\n",
        "# Vectorize the preprocessed comment and make a prediction\n",
        "comment_vector = vectorizer.transform([preprocessed_comment])\n",
        "prediction = model.predict(comment_vector)\n",
        "print(\"Prediction result:\", prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8keLyMD_aSg"
      },
      "source": [
        "Download the CSV files via this link: [Kaggle Dataset](https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/data)\n",
        "\n",
        "Then use Google Colab File Explorer:\n",
        "\n",
        "\n",
        "*   On the left sidebar, click the Files tab.\n",
        "*   Click the Upload button and select the files you want to upload.\n",
        "*   Once uploaded, the files will appear in the file explorer.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTY0yHas95WH",
        "outputId": "fb6ddeaa-ead4-419e-e858-018cede4bef7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_data\n",
            "                 id                                       comment_text  toxic  \\\n",
            "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
            "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
            "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
            "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
            "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
            "\n",
            "   severe_toxic  obscene  threat  insult  identity_hate  \n",
            "0             0        0       0       0              0  \n",
            "1             0        0       0       0              0  \n",
            "2             0        0       0       0              0  \n",
            "3             0        0       0       0              0  \n",
            "4             0        0       0       0              0  \n",
            "test_data\n",
            "                 id                                       comment_text\n",
            "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
            "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
            "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
            "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
            "4  00017695ad8997eb          I don't anonymously edit articles at all.\n",
            "test_labels\n",
            "                 id  toxic  severe_toxic  obscene  threat  insult  \\\n",
            "0  00001cee341fdb12     -1            -1       -1      -1      -1   \n",
            "1  0000247867823ef7     -1            -1       -1      -1      -1   \n",
            "2  00013b17ad220c46     -1            -1       -1      -1      -1   \n",
            "3  00017563c3f7919a     -1            -1       -1      -1      -1   \n",
            "4  00017695ad8997eb     -1            -1       -1      -1      -1   \n",
            "\n",
            "   identity_hate  \n",
            "0             -1  \n",
            "1             -1  \n",
            "2             -1  \n",
            "3             -1  \n",
            "4             -1  \n"
          ]
        }
      ],
      "source": [
        "# Load the files into pandas DataFrames\n",
        "train_data = pd.read_csv('csv-files/train.csv')\n",
        "test_data = pd.read_csv('csv-files/test.csv')\n",
        "test_labels = pd.read_csv('csv-files/test_labels.csv')\n",
        "\n",
        "# Dislay the first few rows of the datasets\n",
        "print(\"train_data\")\n",
        "print(train_data.head())\n",
        "print(\"test_data\")\n",
        "print(test_data.head())\n",
        "print(\"test_labels\")\n",
        "print(test_labels.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "toxic\n",
            "0    144277\n",
            "1     15294\n",
            "Name: count, dtype: int64\n",
            "severe_toxic\n",
            "0    157976\n",
            "1      1595\n",
            "Name: count, dtype: int64\n",
            "obscene\n",
            "0    151122\n",
            "1      8449\n",
            "Name: count, dtype: int64\n",
            "threat\n",
            "0    159093\n",
            "1       478\n",
            "Name: count, dtype: int64\n",
            "insult\n",
            "0    151694\n",
            "1      7877\n",
            "Name: count, dtype: int64\n",
            "identity_hate\n",
            "0    158166\n",
            "1      1405\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(train_data['toxic'].value_counts())\n",
        "print(train_data['severe_toxic'].value_counts())\n",
        "print(train_data['obscene'].value_counts())\n",
        "print(train_data['threat'].value_counts())\n",
        "print(train_data['insult'].value_counts())\n",
        "print(train_data['identity_hate'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FegpP6Kf7PRs"
      },
      "source": [
        "# Model Building and Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZDhB2k-DI78"
      },
      "source": [
        "**Training data set:** This is the largest subset used to train the model by adjusting its parameters. It helps the model learn the underlying patterns in the data.\n",
        "\n",
        "\n",
        "**Validation data set:** We use this set to provide an unbiased evaluation of the model during the training phase.\n",
        "Credit: [link text](https://kili-technology.com/training-data/training-validation-and-test-sets-how-to-split-machine-learning-data#:~:text=Training%20data%20set%3A%20This%20is,model%20during%20the%20training%20phase.)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "[RandomForestClassifier scikit](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
        "\n",
        "[XGBoost\n",
        "](https://xgboost.readthedocs.io/en/latest/python/python_api.html)\n",
        "\n",
        "[More](https://stackoverflow.com/questions/45251126/deprecation-warning-on-xgboost-sklearn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8D7Lc3exBHSw",
        "outputId": "92040713-6066-42ea-f038-81a83e2aa5ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [00:02:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [00:02:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [00:03:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [00:03:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [00:03:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [00:04:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Set Results\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        toxic       0.90      0.60      0.72      3056\n",
            " severe_toxic       0.49      0.30      0.37       321\n",
            "      obscene       0.88      0.70      0.78      1715\n",
            "       threat       0.54      0.28      0.37        74\n",
            "       insult       0.77      0.57      0.65      1614\n",
            "identity_hate       0.60      0.28      0.38       294\n",
            "\n",
            "    micro avg       0.84      0.59      0.69      7074\n",
            "    macro avg       0.70      0.46      0.55      7074\n",
            " weighted avg       0.83      0.59      0.69      7074\n",
            "  samples avg       0.05      0.05      0.05      7074\n",
            "\n",
            "Test Set Results\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        toxic       0.63      0.69      0.66      6090\n",
            " severe_toxic       0.35      0.44      0.39       367\n",
            "      obscene       0.68      0.67      0.67      3691\n",
            "       threat       0.50      0.36      0.41       211\n",
            "       insult       0.67      0.58      0.62      3427\n",
            "identity_hate       0.63      0.37      0.46       712\n",
            "\n",
            "    micro avg       0.64      0.63      0.64     14498\n",
            "    macro avg       0.58      0.52      0.54     14498\n",
            " weighted avg       0.64      0.63      0.63     14498\n",
            "  samples avg       0.06      0.06      0.06     14498\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Apply preprocessing\n",
        "train_data['comment_text'] = train_data['comment_text'].apply(preprocess_text)\n",
        "test_data['comment_text'] = test_data['comment_text'].apply(preprocess_text)\n",
        "\n",
        "# Vectorize the text data\n",
        "vectorizer = TfidfVectorizer(max_features=10000)\n",
        "X_train = vectorizer.fit_transform(train_data['comment_text'])\n",
        "y_train = train_data[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the RandomForestClassifier and XGBoostClassifier models\n",
        "# rf_model = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10, class_weight='balanced')\n",
        "rf_model = RandomForestClassifier(\n",
        "    random_state=42, \n",
        "    n_estimators=100, \n",
        "    max_depth=10, \n",
        "    class_weight='balanced'  # Adjusts weights inversely proportional to class frequencies\n",
        ")\n",
        "\n",
        "xgb_model = XGBClassifier(eval_metric='logloss', use_label_encoder=False)\n",
        "\n",
        "# Combine the models in a VotingClassifier\n",
        "ensemble_model = VotingClassifier(estimators=[\n",
        "    ('rf', rf_model),\n",
        "    ('xgb', xgb_model)\n",
        "], voting='soft')\n",
        "\n",
        "multi_output_model = MultiOutputClassifier(ensemble_model)\n",
        "\n",
        "# Fit the ensemble model\n",
        "multi_output_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "y_val_pred = multi_output_model.predict(X_val)\n",
        "print('Validation Set Results')\n",
        "print(classification_report(y_val, y_val_pred, target_names=y_train.columns, zero_division=0))\n",
        "\n",
        "# Preprocess test data\n",
        "X_test = vectorizer.transform(test_data['comment_text'])\n",
        "y_test_pred = multi_output_model.predict(X_test)\n",
        "\n",
        "# Filter out test labels with -1\n",
        "valid_indices = test_labels[(test_labels[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']] != -1).all(axis=1)].index\n",
        "y_test_true = test_labels.loc[valid_indices, ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values\n",
        "y_test_true[y_test_true == -1] = 0\n",
        "y_test_pred_filtered = y_test_pred[valid_indices]\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "print('Test Set Results')\n",
        "print(classification_report(y_test_true, y_test_pred_filtered, target_names=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], zero_division=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QesNyHTVpCyM"
      },
      "source": [
        "**Toxic and Obscene** classes have relatively high precision and recall, indicating good performance.\n",
        "\n",
        "\n",
        "**Severe Toxic, Threat, and Identity Hate** classes have lower precision and recall, indicating that the model struggles with these categories, likely due to class imbalance or insufficient distinctive features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
