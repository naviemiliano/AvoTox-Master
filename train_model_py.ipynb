{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bezjUlm370ag"
      },
      "source": [
        "# **AVOTOX**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASy4hXNp7Jc4"
      },
      "source": [
        "# Data Collection and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "o3028HmoZm5i",
        "outputId": "5ebcc2dc-7c65-445d-cf18-2b531c0615b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn==1.2.2 in /opt/anaconda3/lib/python3.11/site-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn==1.2.2) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn==1.2.2) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn==1.2.2) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn==1.2.2) (2.2.0)\n",
            "Requirement already satisfied: pandas==2.0.3 in /opt/anaconda3/lib/python3.11/site-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas==2.0.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas==2.0.3) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas==2.0.3) (2023.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /opt/anaconda3/lib/python3.11/site-packages (from pandas==2.0.3) (1.26.4)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas==2.0.3) (1.16.0)\n",
            "Requirement already satisfied: contractions in /opt/anaconda3/lib/python3.11/site-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /opt/anaconda3/lib/python3.11/site-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /opt/anaconda3/lib/python3.11/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /opt/anaconda3/lib/python3.11/site-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
            "Requirement already satisfied: xgboost in /opt/anaconda3/lib/python3.11/site-packages (2.1.0)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.11/site-packages (from xgboost) (1.11.4)\n",
            "Requirement already satisfied: imbalanced-learn in /opt/anaconda3/lib/python3.11/site-packages (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /opt/anaconda3/lib/python3.11/site-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from imbalanced-learn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from imbalanced-learn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from imbalanced-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from imbalanced-learn) (2.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn==1.2.2\n",
        "!pip install pandas==2.0.3\n",
        "!pip install contractions\n",
        "!pip install xgboost\n",
        "!pip install imbalanced-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MYz9CEs1-XiP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import contractions\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7q0kN7SQV0z"
      },
      "source": [
        "**stopwords** are words that are filtered out of natural language data do to be considred unimportant (during or after processing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63PJERkcjfb4",
        "outputId": "f26b2d02-40fe-4504-aedd-88e967ce87dd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/heidyhernandez/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/heidyhernandez/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'vectorizer.pkl'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvectorizer.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulti_output_model.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'vectorizer.pkl'"
          ]
        }
      ],
      "source": [
        "vectorizer = joblib.load('vectorizer.pkl')\n",
        "model = joblib.load('multi_output_model.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjh_-ZrPj6LD"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "  text = text.lower()\n",
        "  # Expand contractions\n",
        "  text = contractions.fix(text)\n",
        "  # Remove HTML tags\n",
        "  text = re.sub(r'<.*?>', '', text)\n",
        "  # Remove URLs\n",
        "  text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "  # Remove email addresses\n",
        "  text = re.sub(r'\\S+@\\S+', '', text)\n",
        "  # Remove hashtags\n",
        "  text = re.sub(r'#\\w+', '', text)\n",
        "  # Remove mentions\n",
        "  text = re.sub(r'@\\w+', '', text)\n",
        "  # Remove special characters except punctuation\n",
        "  text = re.sub(r'[^a-zA-Z\\s.,]', '', text)\n",
        "  # Remove extra whitespace\n",
        "  text = re.sub(r'\\s+', ' ', text).strip()\n",
        "  # Tokenize text\n",
        "  words = text.split()\n",
        "  # Remove stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  words = [word for word in words if word not in stop_words]\n",
        "  # Lemmatize words\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  words = [lemmatizer.lemmatize(word) for word in words]\n",
        "  # Join words to a single string\n",
        "  text = ' '.join(words)\n",
        "  # print(text)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction result: [[1 0 1 0 1 0]]\n"
          ]
        }
      ],
      "source": [
        "# Test the preprocessing with a sample comment\n",
        "comment = \"FUCK YOU!\"\n",
        "preprocessed_comment = preprocess_text(comment)\n",
        "\n",
        "# Vectorize the preprocessed comment and make a prediction\n",
        "comment_vector = vectorizer.transform([preprocessed_comment])\n",
        "prediction = model.predict(comment_vector)\n",
        "print(\"Prediction result:\", prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8keLyMD_aSg"
      },
      "source": [
        "Download the CSV files via this link: [Kaggle Dataset](https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/data)\n",
        "\n",
        "Then use Google Colab File Explorer:\n",
        "\n",
        "\n",
        "*   On the left sidebar, click the Files tab.\n",
        "*   Click the Upload button and select the files you want to upload.\n",
        "*   Once uploaded, the files will appear in the file explorer.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTY0yHas95WH",
        "outputId": "fb6ddeaa-ead4-419e-e858-018cede4bef7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_data\n",
            "                 id                                       comment_text  toxic  \\\n",
            "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
            "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
            "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
            "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
            "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
            "\n",
            "   severe_toxic  obscene  threat  insult  identity_hate  \n",
            "0             0        0       0       0              0  \n",
            "1             0        0       0       0              0  \n",
            "2             0        0       0       0              0  \n",
            "3             0        0       0       0              0  \n",
            "4             0        0       0       0              0  \n",
            "test_data\n",
            "                 id                                       comment_text\n",
            "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
            "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
            "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
            "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
            "4  00017695ad8997eb          I don't anonymously edit articles at all.\n",
            "test_labels\n",
            "                 id  toxic  severe_toxic  obscene  threat  insult  \\\n",
            "0  00001cee341fdb12     -1            -1       -1      -1      -1   \n",
            "1  0000247867823ef7     -1            -1       -1      -1      -1   \n",
            "2  00013b17ad220c46     -1            -1       -1      -1      -1   \n",
            "3  00017563c3f7919a     -1            -1       -1      -1      -1   \n",
            "4  00017695ad8997eb     -1            -1       -1      -1      -1   \n",
            "\n",
            "   identity_hate  \n",
            "0             -1  \n",
            "1             -1  \n",
            "2             -1  \n",
            "3             -1  \n",
            "4             -1  \n"
          ]
        }
      ],
      "source": [
        "# Load the files into pandas DataFrames\n",
        "train_data = pd.read_csv('csv-files/train.csv')\n",
        "test_data = pd.read_csv('csv-files/test.csv')\n",
        "test_labels = pd.read_csv('csv-files/test_labels.csv')\n",
        "\n",
        "# Dislay the first few rows of the datasets\n",
        "print(\"train_data\")\n",
        "print(train_data.head())\n",
        "print(\"test_data\")\n",
        "print(test_data.head())\n",
        "print(\"test_labels\")\n",
        "print(test_labels.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                      id                                       comment_text  \\\n",
            "139074  e83c7bb704364436  Article needs improvement \\n\\nArticle is just ...   \n",
            "73305   c421c5c8f1f595e7  I Have a Hacker with me. \\n\\nIf you continue t...   \n",
            "60861   a2f2d8a7c37c4d83                           This is a sock of banned   \n",
            "123192  930f827d1df03973  Bilateral relationships need searching \\n\\nThe...   \n",
            "70314   bc1eef29e9c6c4a2  \"\\n\\n Personally speaking (of course) I think ...   \n",
            "88892   edd28981dd206902  \"\\n\\n th \\n\\n  th thanks for deleting my fucki...   \n",
            "112321  58dcd559878c0190  Hello\\n\\nI just got a final warning??? from a ...   \n",
            "20361   35c28ee12bdfc238                      what the fuck i do not get it   \n",
            "89237   eeb6e7273ff2d958  There are two sources already provided for the...   \n",
            "20240   3573bae301c74350  Please read up on Wikipedia's core policies be...   \n",
            "9750    19d2c085364d1ca4  Might want to point out experts 99.9% of biolo...   \n",
            "8754    173f912659fd0ca1  There are many biographies on WP that do not m...   \n",
            "56039   95bd5fa7d1a2f3da  To save anyone else from following this rabbit...   \n",
            "86409   e71bf809c83f5db4  Chauntee Schuler is  on contract. \\n\\nThe actr...   \n",
            "77709   d029e281145db352  \" January 2014 (UTC)\\n. Looks like 166.147.0.0...   \n",
            "139454  ea680b2943aab4d0  :The Nature of 22-Pistepirkko\\n\\nA tag has bee...   \n",
            "141351  f43c9313675e5f4b  Blanking one's talk page isn't vandalism \\n\\nA...   \n",
            "16771   2c3a7f1805e1a329  Foxhunt99, from what you say, is it not the ca...   \n",
            "114988  66ee459652935faa  Theatron \\n\\nUnfortunately, this was the tip o...   \n",
            "19438   3351192a7f21b88f  \"\\n\\nAbsolutely oppose.  Not enough content no...   \n",
            "156749  d30709afc1b11b2c  The submission was deleted because it had no c...   \n",
            "109825  4b62035d306dfd8a  Let the record show that  has once again rever...   \n",
            "132830  c6af3f28d8289f11  \" As well as the \"\"Wentworth\"\" Miller article,...   \n",
            "22378   3b0e1b13380cdd7a  I fuck niggas! I fuck niggas! I fuck niggas! I...   \n",
            "132941  c7375a6eca8f6b61  kiss my ass jehochman - i have many ip's, have...   \n",
            "11512   1e6bfa224a445a9a  Tadeusz Kobylinski \\n\\nKobylinski left Poland ...   \n",
            "111514  548f536fa85e6963  Edit request on 7 April 2012 \\n\\nThere is a pa...   \n",
            "66209   b118f2f581fde449  \"\\n\\nFor all the politeness I dealt you, I hav...   \n",
            "90531   f23c0f9df1fef54e  Addendum: I must have written this at the same...   \n",
            "80691   d7de69a49d60504b  Work on your spelling Jay.  It is funny as hel...   \n",
            "125082  9d0e43a51d4d343f  \"\\n\\nSorry, it's just that I qualify assessmen...   \n",
            "68709   b7cb9d4eedcd766f  Kiss meee arse \\n\\nLike its SOOO convincing wh...   \n",
            "52984   8d9aa05fdbedece4  Yes Hamas claim of 1000 soldier dead should be...   \n",
            "106314  38d1ca6018ce32b1  \"\\n\\n Unreferenced \\n\\nWhat in particular is u...   \n",
            "142827  fbda290a8e32c8e3  Chemistry \\n\\nI have added a detailed list of ...   \n",
            "16558   2ba88b97a9bff763  \"\\nThis guy dosen't know when to give up, I'll...   \n",
            "150417  6bfd9b7a88fbdec0  \"\\n\\nIf you'd actually bothered to read the ar...   \n",
            "73344   c43a85d49697a3ac  \"\\n@Marketdiamond: thanks!  In this case, I al...   \n",
            "21957   39d9255d1dd72e71  Addendum: I've emailed my concerns regarding t...   \n",
            "23299   3d8a94f8cab4f449  Its fucking assholes like you who vandalize wi...   \n",
            "8205    15cabcbe1fba6d70  \"\\n\\n Giuliano Mignini Article \\n\\nDear Slim V...   \n",
            "74632   c7ac8268ea8615e8  \"\\n\\nFergus Graham\\nHeho, I have made two litt...   \n",
            "104739  3052fa35bf1440b9  \"\\n\\n=====\\n\\nThe Wikipedia article on Federal...   \n",
            "23943   3f38095d0e650176  Why I put the cleanup tag on this page\\n\\nThis...   \n",
            "23237   3d62ef8c985f32c2  Sorry if I'm wrong about this, but it was my u...   \n",
            "80453   d746256a65c83de9  \"\\n\\nby the way if  you have son or daughter, ...   \n",
            "35595   5f202803c03e6c91  \"\\nMeg: Happy Saturday, January 10th !!! These...   \n",
            "26151   45329cdbbade8aac  \"::You have some flaws. When 2 Brahmans marry ...   \n",
            "143677  00f09d1da6e5d996  June 2006 (UTC)\\n\\nOn top of everything, your ...   \n",
            "98734   102acabf4f336eef  Wait a Hot Second\\nPlease allow me to talk to ...   \n",
            "\n",
            "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
            "139074      0             0        0       0       0              0  \n",
            "73305       1             0        1       1       1              0  \n",
            "60861       0             0        0       0       0              0  \n",
            "123192      0             0        0       0       0              0  \n",
            "70314       0             0        0       0       0              0  \n",
            "88892       1             0        1       0       1              0  \n",
            "112321      0             0        0       0       0              0  \n",
            "20361       1             0        1       0       0              0  \n",
            "89237       0             0        0       0       0              0  \n",
            "20240       0             0        0       0       0              0  \n",
            "9750        0             0        0       0       0              0  \n",
            "8754        0             0        0       0       0              0  \n",
            "56039       0             0        0       0       0              0  \n",
            "86409       0             0        0       0       0              0  \n",
            "77709       0             0        0       0       0              0  \n",
            "139454      0             0        0       0       0              0  \n",
            "141351      0             0        0       0       0              0  \n",
            "16771       0             0        0       0       0              0  \n",
            "114988      0             0        0       0       0              0  \n",
            "19438       0             0        0       0       0              0  \n",
            "156749      0             0        0       0       0              0  \n",
            "109825      0             0        0       0       0              0  \n",
            "132830      0             0        0       0       0              0  \n",
            "22378       1             1        1       0       1              1  \n",
            "132941      1             0        1       0       1              0  \n",
            "11512       0             0        0       0       0              0  \n",
            "111514      0             0        0       0       0              0  \n",
            "66209       0             0        1       0       1              0  \n",
            "90531       0             0        0       0       0              0  \n",
            "80691       1             0        0       0       1              0  \n",
            "125082      0             0        0       0       0              0  \n",
            "68709       1             0        1       0       0              0  \n",
            "52984       0             0        0       0       0              0  \n",
            "106314      0             0        0       0       0              0  \n",
            "142827      0             0        0       0       0              0  \n",
            "16558       0             0        0       0       0              0  \n",
            "150417      0             0        0       0       0              0  \n",
            "73344       0             0        0       0       0              0  \n",
            "21957       0             0        0       0       0              0  \n",
            "23299       1             1        1       0       1              0  \n",
            "8205        0             0        0       0       0              0  \n",
            "74632       0             0        0       0       0              0  \n",
            "104739      0             0        0       0       0              0  \n",
            "23943       0             0        0       0       0              0  \n",
            "23237       0             0        0       0       0              0  \n",
            "80453       0             0        0       0       0              0  \n",
            "35595       0             0        0       0       0              0  \n",
            "26151       0             0        0       0       0              0  \n",
            "143677      0             0        0       0       0              0  \n",
            "98734       0             0        0       0       0              0  \n"
          ]
        }
      ],
      "source": [
        "print(train_data.sample(50))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 id                                       comment_text\n",
            "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
            "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
            "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
            "3  00017563c3f7919a  :If you have a look back at the source, the in...\n"
          ]
        }
      ],
      "source": [
        "print(test_data[:4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "toxic\n",
            "0    144277\n",
            "1     15294\n",
            "Name: count, dtype: int64\n",
            "severe_toxic\n",
            "0    157976\n",
            "1      1595\n",
            "Name: count, dtype: int64\n",
            "obscene\n",
            "0    151122\n",
            "1      8449\n",
            "Name: count, dtype: int64\n",
            "threat\n",
            "0    159093\n",
            "1       478\n",
            "Name: count, dtype: int64\n",
            "insult\n",
            "0    151694\n",
            "1      7877\n",
            "Name: count, dtype: int64\n",
            "identity_hate\n",
            "0    158166\n",
            "1      1405\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(train_data['toxic'].value_counts())\n",
        "print(train_data['severe_toxic'].value_counts())\n",
        "print(train_data['obscene'].value_counts())\n",
        "print(train_data['threat'].value_counts())\n",
        "print(train_data['insult'].value_counts())\n",
        "print(train_data['identity_hate'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FegpP6Kf7PRs"
      },
      "source": [
        "# Model Building and Training - FINISH THE CODE :D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZDhB2k-DI78"
      },
      "source": [
        "**Training data set:** This is the largest subset used to train the model by adjusting its parameters. It helps the model learn the underlying patterns in the data.\n",
        "\n",
        "\n",
        "**Validation data set:** We use this set to provide an unbiased evaluation of the model during the training phase.\n",
        "Credit: [link text](https://kili-technology.com/training-data/training-validation-and-test-sets-how-to-split-machine-learning-data#:~:text=Training%20data%20set%3A%20This%20is,model%20during%20the%20training%20phase.)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "[RandomForestClassifier scikit](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
        "\n",
        "[XGBoost\n",
        "](https://xgboost.readthedocs.io/en/latest/python/python_api.html)\n",
        "\n",
        "[More](https://stackoverflow.com/questions/45251126/deprecation-warning-on-xgboost-sklearn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8D7Lc3exBHSw",
        "outputId": "92040713-6066-42ea-f038-81a83e2aa5ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [23:51:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [23:52:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [23:52:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [23:52:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [23:53:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [23:53:45] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Set Results\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        toxic       0.90      0.61      0.73      3056\n",
            " severe_toxic       0.49      0.30      0.37       321\n",
            "      obscene       0.88      0.72      0.79      1715\n",
            "       threat       0.56      0.27      0.36        74\n",
            "       insult       0.77      0.58      0.66      1614\n",
            "identity_hate       0.63      0.28      0.39       294\n",
            "\n",
            "    micro avg       0.84      0.60      0.70      7074\n",
            "    macro avg       0.71      0.46      0.55      7074\n",
            " weighted avg       0.83      0.60      0.69      7074\n",
            "  samples avg       0.05      0.05      0.05      7074\n",
            "\n",
            "Test Set Results\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        toxic       0.64      0.70      0.67      6090\n",
            " severe_toxic       0.37      0.41      0.39       367\n",
            "      obscene       0.67      0.68      0.68      3691\n",
            "       threat       0.50      0.35      0.41       211\n",
            "       insult       0.68      0.58      0.63      3427\n",
            "identity_hate       0.63      0.37      0.47       712\n",
            "\n",
            "    micro avg       0.65      0.64      0.64     14498\n",
            "    macro avg       0.58      0.52      0.54     14498\n",
            " weighted avg       0.65      0.64      0.64     14498\n",
            "  samples avg       0.06      0.06      0.06     14498\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Apply preprocessing\n",
        "________['comment_text'] = train_data['comment_text'].apply(________)\n",
        "________['comment_text'] = test_data['comment_text'].apply(________)\n",
        "\n",
        "# Vectorize the text data\n",
        "vectorizer = TfidfVectorizer(max_features=10000)\n",
        "X_train = vectorizer.fit_transform(________)\n",
        "\n",
        "y_train = train_data[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "# HINT: think about the train variables \n",
        "X_train, X_val, y_train, y_val = train_test_split(________, ________, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the RandomForestClassifier and XGBoostClassifier models\n",
        "rf_model = ________(\n",
        "    random_state=42, \n",
        "    n_estimators=100, \n",
        "    max_depth=10, \n",
        "    class_weight='balanced'  # Adjusts weights inversely proportional to class frequencies\n",
        ")\n",
        "\n",
        "xgb_model = ________(eval_metric='logloss', use_label_encoder=False)\n",
        "\n",
        "# Combine the models in a VotingClassifier\n",
        "ensemble_model = ________(estimators=[\n",
        "    ('rf', ________),\n",
        "    ('xgb', ________)\n",
        "], voting='soft')\n",
        "\n",
        "multi_output_model = MultiOutputClassifier(________)\n",
        "\n",
        "# Fit the ensemble model\n",
        "# HINT: think about the train variables \n",
        "multi_output_model.fit(________, ________)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "y_val_pred = multi_output_model.predict(________)\n",
        "print('Validation Set Results')\n",
        "# HINT: sklearn.metrics.classification_report(y_true, y_pred, *, labels=None, target_names=None, \n",
        "# sample_weight=None, digits=2, output_dict=False, zero_division='warn')[source]\n",
        "# SEARCH UP: classification_report for more info :D\n",
        "print(classification_report(y_val, ________, target_names=y_train.columns, zero_division=________))\n",
        "\n",
        "# Preprocess test data\n",
        "X_test = vectorizer.transform(test_data['comment_text'])\n",
        "y_test_pred = multi_output_model.predict(________)\n",
        "\n",
        "# Filter out test labels with -1 \n",
        "# HINT: What can get you the index attribute\n",
        "valid_indices = test_labels[(test_labels[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']] != -1).all(axis=1)].________\n",
        "y_test_true = test_labels.loc[valid_indices, ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values\n",
        "# HINT: Replace with 0\n",
        "y_test_true[y_test_true == ________] = ________\n",
        "y_test_pred_filtered = y_test_pred[________]\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "print('Test Set Results')\n",
        "print(classification_report(y_test_true, ________, target_names=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], zero_division=________))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QesNyHTVpCyM"
      },
      "source": [
        "**Toxic and Obscene** classes have relatively high precision and recall, indicating good performance.\n",
        "\n",
        "\n",
        "**Severe Toxic, Threat, and Identity Hate** classes have lower precision and recall, indicating that the model struggles with these categories, likely due to class imbalance or insufficient distinctive features."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
